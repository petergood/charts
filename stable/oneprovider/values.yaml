# Default values for oneprovider.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image: onedata/oneprovider:18.02.1 
imagePullPolicy: IfNotPresent
serviceType: ClusterIP

onedata_cli:
  image: onedata/rest-cli:18.02.1
  imagePullPolicy: IfNotPresent

wait_for:
  image: onedata/k8s-wait-for:v1.2
  imagePullPolicy: IfNotPresent

# Specifies if ready check job should be created.
oneprovider_ready_check:
  enabled: true

# Wait for Onezone service to be ready before starting.
wait-for-onezone:
  enabled: true

# To register to Onezone, Oneprovider needs registration token
onezone-registration-token:
  # Automatically acquire provider registration token prior to starting onezone
  enabled: true
  # Or provide a token manually
  value: ""

# When running multiple copies of oneprovider as a part of a helm release
# each of oneprovider charts may be give a suffix, so that service names
# do not collide. This is propagated to all oneprovider chart dependencies.
suffix: &providerSuffix ''

# An address of Onezone service (mandatory)
# support for multiple scenarios, see examples bellow
onezone_service_url: &onezone_service_url
  # 1. no address is needed
  # the assumption is made that onezone is part of the same helm release
  # the disableSuffix flag, prevents suffix to be appended to onezone service name
  type: auto-generate
  disableSuffix: true

  # 2. a full address of an onezone (without http(s))
  # note that external onezones will not be able to communicate to k8s deployed oneprovider
  # type: http
  # address: beta.onedata.org

  # 3. only service name is sufficient, the namespace parameter is optional
  # type: k8s-service
  # service_name: rc14-onezone
  # namespace: default

# oneprovider name, if empty defaults to a chart name
name: ''

# Resources requested by oneprovider
cpu: 1.5
memory: 4Gi

# Coordinates of a Oneprovider on a onezone map
geoLatitude: 55.333
geoLongitude: 55.333

# Oneprovider admin email
adminEmail: "getting-started@onedata.org"

# Log level of the processes in the container
log_level: "info"

# Prevent oneprovider container to exit when container entrypoint fails
onepanel_debug_mode: false

# Enable loading oneprovider configuration from ONEPROVIDER_CONFIG env variable
onepanel_batch_mode_enabled: true

# If enabled, a new web cert will be generated with CN matching the
#   ONEPANEL_GENERATED_CERT_DOMAIN and signed by OnedataTestWebServerCa
# NOTE: The generation will be performed upon every startup, any
#   existing certs will be backed up and placed in the same directory.
# WARNING: This functionality is devised for test purposes and must not
#   be used in production.
onepanel_generate_test_web_cert: true

# The generated test web cert will be issued for below domain.
onepanel_generated_cert_domain: ""

# If enabled, onepanel will trust any server that has a cert signed by
#   the OnedataTestWebServerCa.
# WARNING: This functionality is devised for test purposes and must not
#   be used in production.
onepanel_trust_test_ca: true

# When true, all GUIs hosted in this cluster will print
# debug logs to browser console.
guiDebugMode: false

# Number of nodes (pod replicas) for of this provider
# their indexes will be assigned FROM 0 (ZERO!)
# up to, but not including the count value
oneprovider_nodes_count: 1

# Assign oneprovider services to oneprovider instances
# you can use values form the rage <0,oneprovider_nodes_count)
# by default the node with the highest index (oneprovider_nodes_count-1)
# is configured as a mainNode
# If a service list is empty, all available nodes are assigned that service. 
cluster_config:
  managers: [ ]
  workers: [ ]
  databases: [ ]

# Oneprovider entrypoint waits for external rsync to finish updating 
# container files before it continues
deployFromSources:
  enabled: false

# Erlang configuration file, which values will override
# respected default values in the app.conf of Onepanel
# The idea is to override as little as possible and allow
# the app.conf evolve on it's own.
panelOverlayConfig: |-
  [].

# Same story as above, just for worker service.
workerOverlayConfig: |-
  [].

# The generalization of nodeSelector.
# Allows for more fine grained controls over which
# nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# List of taints which are tolerated by the pods 
# when nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# Specify a map of key-value pairs. For the pod 
# to be eligible to run on a node, the node
# must have each of the indicated key-value pairs as labels
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

# Generate certificates to access this onezone having trusted https connection
# You need to add cluster root certificate to your system for this to work
generate-certificates: 
  enabled: false
  image: onedata/certificate-init-container:8434eb0
  imagePullPolicy: IfNotPresent

# Create a PVC and persist /volumes/persistence
# Atm. PVC expects to find a matching PV, which is assumed is 
# to be local persistent volume. Those should be created and managed using this:
# https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume/helm
persistence:
  enabled: false
  size: 20Gi
  accessModes:
    - ReadWriteOnce
  cleanUpJob:
    enabled: true
    image: onedata/k8s-wait-for:v1.2
    imagePullPolicy: IfNotPresent
  
# If set true, oneprovider will not be deployed on the same node as other oneproviders
# or onezones that are part of this deployment
onedata_anti_affinity_enabled: false

# Those credentials will be used by all onepanel REST operations etc.
onepanel_emergency_account: &onepanel_emergency_account
  name: onepanel
  password: password

# List of oneprovider onepanel users with administrative privileges
onepanel_admin_users:
  - name: admin
    password: password
  - *onepanel_emergency_account

# List of oneprovider onepanel regular users
onepanel_users:
  - name: user
    password: password

# Create oneclient instance that will connect to this oneprovider
oneclient:
  enabled: false
  onezone_service_url: *onezone_service_url
  suffix: *providerSuffix
  directIO:
    nfs:
      enable: true
      mounts: *nfs-exports
  token_dispenser_service_url:
    type: auto-generate
    disableSuffix: true

# Create jupyter notebook with oneclient-fs that will connect to this oneprovider
jupyter-notebook:
  enabled: false
  onezone_service_url: *onezone_service_url
  suffix: *providerSuffix

# Create onedata-cli instance that will have a context of this oneprovider
# and this provider onezone
onedata-cli:
  enabled: false
  suffix: *providerSuffix
  onezone_service_url: *onezone_service_url

# Luma configuration shared among all the storages
luma:
  enabled: false
  suffix: *providerSuffix
  lumaCacheTimeout: 5
  lumaApiKey: example_api_key
  # Enable luma for a storage type
  # atm. luma only supports posix
  posix:
    enabled: &luma_enabled_posix false
  s3:
    enabled: &luma_enabled_s3 false
  ceph:
    enabled: &luma_enabled_ceph false
  swift:
    enabled: &luma_enabled_swift false
  gluster:
    enabled: &luma_enabled_gluster false
  nfs:
    enabled: &luma_enabled_nfs false
  webdav:
    enabled: &luma_enabled_webdav false

# Create a extra containers in the same pod as Oneprovider with a dataset
# that can be imported using data sync space support option.
# Will be added to the Oneprovider as POSIX storage
# the mount points are created in /volumes/<storage_name>
# eg. /volumes/volume-data-sync-rw
volume-data-sync:
  enabled: false
  volumes:
  - name: volume-data-sync-rw
    luma-enabled: *luma_enabled_posix
    image: onedata/eo-data-mock:Landsat-5-sample-latest
    imagePullPolicy: IfNotPresent
    dataPath: /data
    readOnly: false
    mountPermissions: 777
    permissions: 
      - path: Landsat-5
        # chown -R command parameters
        # the root of the path is dataPath
        user: 40001 #admin
        group: 42001 #alpha
      - path: Landsat-5/TM/L1T/2010/06/13
        user: 40001 #user
        group: 42001 #beta
      - path: Landsat-5/TM/L1T/2010/06/21
        user: 40001 #user
        group: 42001 #gamma

  - name: volume-data-sync-ro
    luma-enabled: *luma_enabled_posix
    image: onedata/eo-data-mock:Landsat-5-sample-latest
    imagePullPolicy: IfNotPresent
    dataPath: /data
    readOnly: true

# Posix local storage, it will be automatically disable if you deploy multi-node oneprovider installation
posix:
  enabled: true
  storagePath: /volumes/persistence/storage
  luma-enabled: *luma_enabled_posix
  readOnly: false

# Create S3 storage server and connect with oneprovider
volume-s3:
  enabled: false
  suffix: *providerSuffix
  insecure: true
  luma-enabled: *luma_enabled_s3
  readOnly: false

# Create CEPH storage server and connect with oneprovider
volume-ceph:
  enabled: false
  suffix: *providerSuffix
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_ceph

# Create NFS storage server and connect with oneprovider with tested default values
volume-nfs:
  enabled: true
  suffix: *providerSuffix
  exports: &nfs-exports
  - name: empty
    luma-enabled: *luma_enabled_nfs
    readOnly: false
    storageClaim: 1T
  mount_options:
    - soft
    - intr
    - timeo=5
    - retrans=2
  deployAsPreInstallHook: true
  cleanUpJob:
    enabled: true
  readOnly: false

# Create gluster storage server and connect with oneprovider with tested default values
volume-gluster:
  enabled: false
  suffix: *providerSuffix
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_gluster

volume-swift:
  enabled: false
  suffix: *providerSuffix
  tenantName: test
  username: tester
  password: testing
  containerName: test
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_swift
  
volume-webdav:
  enabled: false
  suffix: *providerSuffix
  username: tester
  password: testing
  credentialsType: basic
  rangeWriteSupport: sabredav
  verifyServerCertificate: false
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_webdav
  
volume-dcache:
  enabled: false
  suffix: *providerSuffix
  username: ""
  password: ""
  credentialsType: none
  rangeWriteSupport: none
  verifyServerCertificate: false
  readOnly: true
  insecure: true
  luma-enabled: *luma_enabled_webdav

# This section will be added the end of 'storages' section in oneprovider
# cluster configuration
# avoid storage names: posix, nfs, ceph, s3, gluster, swift
external-storages: {}