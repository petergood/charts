# Default values for oneprovider.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image: onedata/oneprovider:17.06.0-rc1
imagePullPolicy: IfNotPresent
serviceType: ClusterIP

wait_for:
  image: onedata/k8s-wait-for:v1.0.1
  imagePullPolicy: IfNotPresent

# When running multiple copies of oneprovider as a part of a helm release
# each of oneprovider charts may be give a suffix, so that service names
# do not collide. This is propagated to all oneprovider chart dependencies.
suffix: &providerSuffix ''

# An address of Onezone service (mandatory)
# support for multiple scenarios, see examples bellow
onezone_service_url: &onezone_service_url
  # 1. no address is needed
  # the assumption is made that onezone is part of the same helm release
  # the disableSuffix flag, prevents suffix to be appended to onezone service name
  type: auto-generate
  disableSuffix: true

  # 2. a full address of an onezone (without http(s))
  # note that external onezones will not be able to communicate to k8s deployed oneprovider
  # type: http
  # address: beta.onedata.org

  # 3. only service name is sufficient, the namespace parameter is optional
  # type: k8s-service
  # service_name: rc14-onezone
  # namespace: default

# oneprovider name, if empty defaults to a chart name
name: ''

# Resources requested by oneprovider
cpu: 1.5
memory: 4Gi

# Coordinates of a Oneprovider on a onezone map
geoLatitude: 55.333
geoLongitude: 55.333

# Log level of the processes in the container
log_level: "info"

# Enable loading oneprovider configuration from ONEPROVIDER_CONFIG env variable
onepanel_batch_mode_enabled: true

# Number of nodes (pod replicas) for of this provider
# their indexes will be assigned FROM 0 (ZERO!)
# up to, but not including the count value
oneprovider_nodes_count: 1

# Assign oneprovider services to oneprovider instances
# you can use values form the rage <0,oneprovider_nodes_count)
# by default the node with the highest index (oneprovider_nodes_count-1)
# is configured as a mainNode
# If a service list is empty, all available nodes are assigned that service. 
cluster_config:
  managers: [ ]
  workers: [ ]
  databases: [ ]

# The gneralization of nodeSelector.
# Allows for moe fine grained controll over which
# nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# List of taints which are tolerated by the pods 
# when nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# Specify a map of key-value pairs. For the pod 
# to be eligible to run on a node, the node 
# must have each of the indicated key-value pairs as labels
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

# Generate certificates to access this onezone having trusted https connection
# You need to add cluster root certificate to your system for this to work
generate-certificates: 
  enabled: false
  image: onedata/certificate-init-container:8434eb0
  
# If set true, oneprovider will not be deployed on the same node as other oneproviders
# or onezones that are part of this deployment
onedata_anti_affinity_enabled: false

# List of oneprovider onepanel users with administrative privileges
onepanel_admin_users:
  - login: admin
    password: password

# List of oneprovider onepanel regular users
onepanel_users:
  - login: user
    password: password

# Create oneclient instance that will connect to this oneprovider
oneclient:
  enabled: false
  onezone_service_url: *onezone_service_url
  suffix: *providerSuffix

# Create onedata-cli instance that will have a context of this oneprovider
# and this provider onezone
onedata-cli:
  enabled: false
  suffix: *providerSuffix
  onezone_service_url: *onezone_service_url

# Luma configuration shared among all the storages
luma:
  enabled: false
  suffix: *providerSuffix
  lumaCacheTimeout: 5
  lumaApiKey: example_api_key

  posix:
    enabled: &luma_enabled_posix true
    lowest_uid: 1000
    highest_uid: 65536
    
  s3:
    enabled: &luma_enabled_s3 false
    access_key: accessKey
    secret_key: verySecretKey

  ceph:
    enabled: &luma_enabled_ceph false
    pool_name: test
    username: client.k8s
    key: AQC1oSZZZfucFxAA34MekwoWBm7qpGd0A8u

  swift:
    enabled: &luma_enabled_swift false
    username: swift
    password: swift

  gluster:
    enabled: &luma_enabled_gluster false

  nfs:
    enabled: &luma_enabled_nfs false
    lowest_uid: 1000
    highest_uid: 65536

  nfs:
    enabled: &luma_enabled_nfs false
    lowest_uid: 1000
    highest_uid: 65536

# Posix local storage, it will be automatically disable if you deploy multi-node oneprovider installation
posix:
  enabled: true
  luma-enabled: *luma_enabled_posix

# Create S3 storage server and connect with oneprovider
volume-s3:
  enabled: false
  suffix: *providerSuffix
  insecure: true
  luma-enabled: *luma_enabled_s3

# Create CEPH storage server and connect with oneprovider
volume-ceph:
  enabled: false
  suffix: *providerSuffix
  insecure: true
  luma-enabled: *luma_enabled_ceph

# Create NFS storage server and connect with oneprovider with tested default values
volume-nfs:
  enabled: false
  suffix: *providerSuffix
  container_mount_path: /volumes/nfs
  storage_claim: 1T
  mount_options: "soft,intr,timeo=10"
  luma-enabled: *luma_enabled_nfs

# Create gluster storage server and connect with oneprovider with tested default values
volume-gluster:
  enabled: false
  suffix: *providerSuffix
  insecure: true
  luma-enabled: *luma_enabled_gluster

volume-swift:
  enabled: false
  suffix: *providerSuffix
  tenantName: test
  username: tester
  password: testing
  containerName: test
  insecure: true
  luma-enabled: *luma_enabled_swift
